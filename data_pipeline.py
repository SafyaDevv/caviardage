### This script is the main script for this project
### it handles most steps of the data pipeline
### used in streamlit_app.py for displaying data and visualisations

import pandas
from pandas.api.types import is_numeric_dtype
import seaborn as sea
import matplotlib.pyplot as plot
import wordcloud as wc
from collections import Counter
from scipy.stats import normaltest

### DATA INGESTION ###

print("data_pipeline.py: Loading dataset...")
file = "files/data_16k.json"
blackout_df = pandas.read_json(file)
print("data_pipeline.py: Dataset loaded.")

### DATA CLEANING ###

# Poems with non-alphabetical symbols, single random letters, and duplicates are removed
# + track of counts for visualisation

print("data_pipeline.py: Cleaning dataset...")
count_of_poems = len(blackout_df["poem"])

#Cleaning poems that don't match criterias + keeping count of what was removed for visualisation purposes

clean_df = blackout_df[~blackout_df["poem"].str.contains(r"[^a-zA-Z\s]", na=False)]  #alphabetical chars only
count_non_alpha_p = count_of_poems - len(clean_df)

clean_df = clean_df[~clean_df["poem"].str.contains(r"\b(?![aio]\b)[a-z]\b", 
                                                        na=False, case=False)] #no poems with random single letters that aren't words
count_random_letter = count_of_poems - len(clean_df)

clean_df.drop_duplicates(subset=["poem"], inplace=True, keep="first") #if there is duplicates, only keep first occurrence
count_of_duplicate = count_of_poems - len(clean_df)

clean_df["poem"] = clean_df["poem"].str.strip() #remove trailing/leading spaces

clean_df.drop(columns=["grammar-check"], inplace=True) #Dropping grammar-check column as all values are false after cleaning of non-alphabetical symbols

print("data_pipeline.py: Dataset cleaned.")

### DATA PREPROCESSING ###

print("data_pipeline.py: Loading preprocessed dataset with new features...")

#updated dataset n/ new features applied to each poem, done in generate_dataset.py
clean_df_v2 = pandas.read_csv("files/caviardage_dataset.csv") 

clean_df_v2.reset_index(drop=True, inplace=True)

count_unknown_words = len(clean_df) - len(clean_df_v2) #count of rows removed during NLP in pos_handler.pos

print("data_pipeline.py: Preprocessed dataset loaded.")

#getting word frequencies csv generated by generate_frequencies.py
def get_word_frequency_files():
    word_freq_poems = pandas.read_csv("files/poems_word_frequency.csv")
    word_freq_passages = pandas.read_csv("files/passages_word_frequency.csv")
    return word_freq_poems, word_freq_passages

### DATA PROCESSING  & ANALYSIS ###

#helper function used to turn dataframe into dict that can be handled by Counter
#and WorldCloud
def get_frequency_dict(which_text):
    #getting right file based on parameter
    word_freq_poems, word_freq_passages = get_word_frequency_files()
    if(which_text == "poems"):
        wf_df = word_freq_poems
    elif(which_text == "passages"):
        wf_df = word_freq_passages
    else:
        raise Exception("Wrong parameter. 'poems' or 'passages'")

    #turning .csv file back into dictionary that can be handled by WordCloud
    word_frequency = dict(zip(wf_df["word"], wf_df["frequency"]))

    return word_frequency

#function returning most frequent words
#first wrap dict into counter to be able to use most common words function
def get_most_common_words(text, how_many: int):
    frequency_counter = Counter(get_frequency_dict(text))
    most_common_words = frequency_counter.most_common(how_many)
    return most_common_words

#new dataframe used in stacked bar charts with top words 
# in both poems and passages
def get_overall_word_freq(how_many: int):
    df_poems = pandas.DataFrame(get_most_common_words("poems",how_many),
            columns = ["word", "frequency"])
    
    #adding series to indicate where the frequency is from
    df_poems["source"] = "poems"

    df_passage = pandas.DataFrame(get_most_common_words("passages",how_many),
            columns = ["word", "frequency"])
    df_passage["source"] = "passages"

    #joining them together
    frames = [df_passage, df_poems]
    overall_word_freq = pandas.concat(frames)

    return overall_word_freq

#correlation matrix of numeric features before encoding categorical features
def get_correlation_matrix():
    corr = clean_df_v2.corr(numeric_only=True)
    return corr

#function to get counts of each theme
def get_theme_counts():
    themes = clean_df_v2["poem-theme"].unique().tolist()
    theme_count = []

    #setting up 2d array to hold counts
    for theme in themes:
        theme_count.append([theme, 0])

    #counting occurrences of non empty themes
    for entry in clean_df_v2["poem-theme"]:
        if pandas.isna(entry):
            theme_count[12][1] += 1
        else:
            for theme in theme_count:
                if entry == theme[0]:
                    theme[1] += 1

    #pie chart
    plot.figure(figsize=(8,8))
    plot.pie([count[1] for count in theme_count],
                labels=[count[0] for count in theme_count],
                autopct="%1.1f%%",
                startangle=140)
    plot.title("Distribution of Poem Themes", fontsize=14)
    plot.tight_layout(pad=2.0)
    plot.show()
    
    print(theme_count)

#only run when this file is executed directly
if __name__ == "__main__":
    get_theme_counts()
    #find nan cluster ids
    print(clean_df_v2[clean_df_v2["poem-cluster-id"].isna()])

### VISUALISATION ###
#used in streamlit_app.py and report

'''Function returning a pie chart showing an overview of the data cleaning process
used in streamlit_app.py'''
def plot_cleaning_pie_chart():

    labels = ['Poems with non-alphabetical symbols', 'Poems with single random letters', 'Duplicate poems', 'Poems with unrecognisable words', 'Poems kept in cleaned dataset']
    numbers = (count_non_alpha_p, count_random_letter, count_of_duplicate, count_unknown_words, len(clean_df))
    colours = ["#959595", "#E0BEFF", "#92A2FF","#000000", "#572ba9"]
    explodedSlices = [0.2, 0.2, 0.2, 0.2, 0.1] 

    # Creating plot
    fig = plot.figure(figsize=plot.figaspect(1))
    plot.title("Data Cleaning Overview", fontsize=14)
    plot.pie(numbers,
        labels=None,
        explode=explodedSlices,
        colors=colours,
        autopct="%1.1f%%",
        pctdistance=1.3,
        startangle=160,
        wedgeprops={"linewidth": 1, "edgecolor": "black"},
        textprops={"fontsize": 12}
    )

    plot.legend(labels,
        title="Categories",
        loc="center left",
        bbox_to_anchor=(0.5, 0),
        frameon=True,
        fontsize=8,
        title_fontsize=10)

    plot.tight_layout(pad=1.5)
    #plot.show()
    return fig

# #!!!!!!!! FIRST I NEED TO BUCKET THE SENTIMENT PER CATEGORY (BINNING)
# def plot_sentiment_count():
# sea.countplot(x='')

def get_correlation_heatmap():
    corr = get_correlation_matrix()

    fig = plot.figure(figsize=(6,6))
    sea.heatmap(corr, cmap="coolwarm", annot=True)
    plot.title("Correlation heatmap of numeric features")
    return fig

#function to generate wordcloud based on frequencies csv files
def generate_wordcloud(text, colormap):

    #generating wordcloud
    wordcloud_poem = wc.WordCloud(colormap=colormap).generate_from_frequencies(get_frequency_dict(text))
    fig, ax = plot.subplots(figsize=(10, 5))
    ax.imshow(wordcloud_poem, interpolation='bilinear')
    ax.axis("off")
    return fig

#function returning scatter plots used in streamlit_app.py
def get_scatterplot(x, y, title, color):
    fig, ax = plot.subplots(figsize=(10, 5))
    sea.scatterplot(data = clean_df_v2, x=x, 
                    y=y, ax = ax, color=color)
    ax.set_title(title)
    return fig

#functions to check distribution of numeric columns
def check_uniformity_hist(col, bins=20):
    fig, ax = plot.subplots()
    sea.histplot(col, bins=bins, ax=ax)
    ax.set_title(f"Histogram of {col.name}")
    fig.show()

def check_uniformity_kde(col):
    fig, ax = plot.subplots()
    sea.kdeplot(col, ax=ax, fill=True)
    ax.set_title(f"KDE plot of {col.name}")
    fig.show()

